input_file;output
Gqh2Bf5W4TdK.html;"""{\""goal\"":\""Making the Secret History of the Mongols (SHM) machine-processable and machine-understandable to explore its historical and cultural significance using computational history.\"",\""methods\"":\""The SHM's critical edition by Paul Pelliot and English translation by Igor de Rachewiltz were parsed into the Engineering Historical Memory (EHM) platform, tagged with metadata, and analyzed using HGIS, sentiment analysis, and hierarchical graphs.\"",\""process\"":\""Metadata on geolocation, story types, and chronology was added to SHM sections; EHM tools visualized historical content with interactive infographics; real-time relevant online resources were linked for contextual insights.\"",\""results\"":\""The SHM application enabled scalable analysis, linking SHM with other historical sources on EHM; sentiment analysis and visualization tools provided insights into its narratives, enhancing understanding of its historiographic and philological importance.\""}"""
BuWvtJFxh3wy.html;"""{\""goal\"":\""Developing the Video Reuse Detector (VRD) toolkit to explore visual similarities in audiovisual archives, enhancing the study of video reuse and cultural memory transformation.\"",\""methods\"":\""The VRD integrates machine learning techniques, particularly convolutional neural networks (CNNs) for feature extraction, with the Faiss library for large-scale similarity searches, alongside tools for dataset preparation and visualization.\"",\""process\"":\""The VRD extracts video frames, processes them through EfficientNetB4 CNN to generate visual fingerprints, indexes these fingerprints using Faiss, and applies filtering techniques to refine similarity matches, enabling detailed reuse pattern analysis.\"",\""results\"":\""The VRD successfully identified reuse in archival video datasets, enabling researchers to trace cultural narratives, understand audiovisual heritage transformations, and explore large-scale archives effectively while addressing scalability challenges.\""}"""
69Xry3ztPAk5.html;"""{\""goal\"":\""Exploring the role of the Rotary Club in shaping the transnational public sphere in Republican China (1919-1949) through the analysis of bilingual press representations and computational methods.\"",\""methods\"":\""Utilizing digitized English and Chinese newspapers alongside computational tools such as topic modeling, named entity recognition (NER), GIS, and network analysis to analyze the Rotary Club's activities and public image.\"",\""process\"":\""Constructing bilingual corpora from English and Chinese periodicals, employing topic modeling to uncover themes, using GIS to map transnational activities, and analyzing NER results to trace key actors and organizations across scales.\"",\""results\"":\""Revealed distinct focuses in the English and Chinese presses, reflecting varied reader interests; highlighted the Rotary Club's efforts to promote international peace and local welfare; demonstrated the value of computational approaches in exploring the public sphere.\""}"""
fwpktfFtn5jm.html;"""{\""goal\"":\""Investigating the reporting and composition of the Shenbao (1872-1891) to understand how news defined individuals' notability in late imperial China through digital methodologies.\"",\""methods\"":\""Employing natural language processing (NLP), named entity recognition (NER), network analysis, and topic modeling to process articles and analyze individuals and organizations mentioned in the Shenbao.\"",\""process\"":\""Building a dataset of 130,485 unique articles filtered for quality, applying NER to identify entities like persons and institutions, cleaning data for standardization, and visualizing patterns through network and topic modeling.\"",\""results\"":\""Identified dominance of public officials and institutions in the Shenbao's content, revealing the newspaper's reliance on official sources and highlighting disparities in mentions, with only 3.1% of names appearing more than five times.\""}"""
NpQvUTkRCF5T.html;"""{\""goal\"":\""Introducing the paradigm shift towards computational methodologies in Chinese studies and broader humanities disciplines, highlighting their transformative potential for reorienting digital history.\"",\""methods\"":\""The article discusses advanced computational tools and interdisciplinary approaches, including Monte Carlo simulations, network analysis, topic modeling, hierarchical clustering, and natural language processing. It emphasizes the integration of digital methods with traditional historical analysis to address challenges in data volume, source complexity, and algorithmic biases.\"",\""process\"":\""The study involved curating a diverse corpus of historical data, devising computational workflows for processing and analyzing textual and network data, and applying critical perspectives to algorithmic outputs. Experiments included exploring intertextuality in historical novels, visualizing bureaucratic structures, and analyzing the dynamics of public and private spheres in Chinese history.\"",\""results\"":\""The article highlights breakthroughs in Chinese digital history, such as mapping intertextuality in literature, uncovering hidden rules of bureaucratic systems, and identifying key figures and dynamics in historical networks. It also underscores the importance of transparency and reproducibility, with datasets and methods made publicly accessible.\""}"""
jnkqqTTKW8km.html;"""{\""goal\"":\""Exploring the representation of women in early modern handwritten news using computational techniques to address randomness and incompleteness in historical archives.\"",\""methods\"":\""Combining dynamic system modeling (Random Walks, Markovian simulation), Bayesian inference, and semantic modeling to analyze a collection of Italian manuscript newsletters from the Medici Archive.\"",\""process\"":\""Using computational algorithms to identify the presence and themes associated with women in news, applying semantic space modeling to contextualize narratives, and implementing bootstrapping to simulate possible representations in the lost corpus.\"",\""results\"":\""Highlighted the limited but recurring presence of women in early modern news, the dominance of elite figures, and patterns of representation tied to movement, social roles, and elite contexts, while addressing archival randomness.\""}"""
aADj3Ljjqti2.html;"""{\""goal\"":\""Analyzing the international newspaper coverage of the USS Maine explosion (1898) to uncover national biases and public opinion formation using data-assisted methodologies.\"",\""methods\"":\""Employing text mining, topic modeling, and manual annotation on digitized newspaper corpora from seven countries, alongside computational analysis to extract patterns and trends in opinion-making.\"",\""process\"":\""Constructing a multilingual corpus of 1,627 articles from digitized archives, refining data with keyword searches, analyzing opinion stances through annotated texts, and comparing actor-based stances to identify biases.\"",\""results\"":\""The study revealed varying national perspectives on the cause of the explosion, with some emphasizing accidental causes and others deliberate attacks, highlighting media influence on public opinion and geopolitical narratives.\""}"""
Yn8z7XNPKzy4.html;"""{\""goal\"":\""Visualizing 8,000 doctoral theses at EPFL to explore the institution's academic advising practices and history through advanced data visualization and AI techniques.\"",\""methods\"":\""Analyzing theses using natural language processing, dimensionality reduction techniques (UMAP), and network visualization to create an interactive representation accessible to the public.\"",\""process\"":\""Using textual and metadata analysis to construct a temporal and spatial visualization, integrating TF-IDF for term extraction and dimensionality reduction for mapping similarities among theses.\"",\""results\"":\""Produced interactive visualizations that highlight EPFL's evolution and organizational structure, offering novel insights into academic practices and institutional growth to a broad audience.\""}"""
X3MGSKqAycaT.html;"""{\""goal\"":\""Digitizing and analyzing Bob Damron\u2019s Address Books to study LGBTQ life in the United States (1965-1989) using computational methods and geospatial mapping.\"",\""methods\"":\""Employing geospatial mapping, pattern matching, and computational analysis to transform historical data into visualized trends and detailed maps of LGBTQ spaces across decades.\"",\""process\"":\""Digitizing Damron\u2019s Address Books, transcribing entries, geocoding unclear locations, and integrating the data into interactive visualizations and datasets for researchers and the public.\"",\""results\"":\""Revealed growth and transformation of LGBTQ spaces, identified trends in safety and danger, and provided accessible tools for exploring queer history at scale using computational methods.\""}"""
m734RWDSLo9C.html;"""{\""goal\"":\""Developing imagineRio Narratives, a map-based storytelling tool for creating spatial narratives about the history of Rio de Janeiro, accessible to both experts and amateurs without requiring GIS expertise.\"",\""methods\"":\""Leveraging the historical mapping platform imagineRio, integrating microservices architecture, vector tiles, and bilingual functionality, and employing feedback from user groups for iterative development.\"",\""process\"":\""Built a web-based editor for spatial narratives using KeystoneJS and NextJS, developed tools for embedding maps, annotating geographic features, and incorporating multimedia, and conducted user workshops to refine the platform.\"",\""results\"":\""Launched a user-friendly tool enabling the creation of 140 narratives with over 1,100 annotations, expanding the accessibility of spatial history and fostering a community-driven approach to documenting Rio's past.\""}"""
4ZDXFXQwhoA8.html;"""{\""goal\"":\""Analyzing the discussions within the Delors Committee to explore how economic union topics were addressed during the creation of the European Monetary Union (EMU) using mixed-methods analysis.\"",\""methods\"":\""Combining text mining, topic modeling, network analysis, and close reading of meeting transcripts and tapes to identify discussed topics, participants' contributions, and the extent of attention given to economic union.\"",\""process\"":\""Digitizing meeting records, employing natural language processing for entity recognition, conducting network analysis to map interactions, and applying topic modeling to uncover latent themes in the discussions.\"",\""results\"":\""Revealed that economic union was the second most discussed topic after monetary policy, but key aspects like banking regulation were underrepresented; demonstrated the value of integrating computational and qualitative methods for historical analysis.\""}"""
9HcfToh7EYm8.html;"""{\""goal\"":\""Analyzing changing vocabularies in Dutch parliamentary debates during the cruise missile controversy (1970-1990) to reveal shifts in political discourse influenced by public protests and geopolitical changes.\"",\""methods\"":\""Using Word Embedding Models (WEMs), including the Word2Vec algorithm, to analyze word proximities and their changes over time, complemented by manual annotation to create clusters of related terms.\"",\""process\"":\""Processing a corpus of parliamentary debates from 1970-1990, constructing vectors for nuclear weapons and related terms, and mapping discursive spaces of political parties using dot product metrics to visualize shifts in terminology.\"",\""results\"":\""Identified shifts in political vocabularies towards anti-proliferation, influenced by geopolitical changes and public sentiment, and demonstrated the potential of WEMs for diachronic historical research.\""}"""
m7DWqDjY3hoV.html;"""{\""goal\"":\""Reflecting on the concept of updatism to explore how the Journal of Digital History adapts to an unstable digital environment through continuous updates to its publishing platform.\"",\""methods\"":\""Combining user feedback, co-design approaches, and memory studies to refine navigation tools, enhance functionality, and sustain the journal's platform with features like faceted search and interactive content.\"",\""process\"":\""Implementing discrete and major updates to the platform, integrating feedback from seminars and conferences, and maintaining a GitHub repository for transparency while collaborating with global scholars.\"",\""results\"":\""Enhanced navigation and user interaction through features like faceted search and interactive tables of content, while highlighting updatism as a challenge for sustaining digital publishing in an ever-changing environment.\""}"""
33pRxE2dtUHP.html;"""{\""goal\"":\""Recreating the Kinora motion picture technology using 3D modelling and printing as a heuristic method to explore its historical and technical significance.\"",\""methods\"":\""Combining experimental media archaeology, CAD modelling, rapid prototyping, and interdisciplinary collaboration between media historians and engineers to create a functional Kinora replica.\"",\""process\"":\""Using an original Kinora viewer and reels as models, developing CAD models, validating designs through finite element analysis, and producing parts using 3D printing and desktop manufacturing techniques in the Engineering 3D Lab.\"",\""results\"":\""Produced a functional Kinora replica, providing new insights into its materiality, mechanics, and historical usage while demonstrating the value of interdisciplinary approaches and hands-on experimentation in media archaeology.\""}"""
WBqfZzfi7nHK.html;"""{\""goal\"":\""Analyzing Greek and Latin inscriptions using digital tools to uncover macro-historical patterns of past societies, their organization, and cultural practices across the Mediterranean Basin (8th century BC to 8th century AD).\"",\""methods\"":\""Combining open-source tools, reproducible pipelines, and digital infrastructures to align datasets, handle incomplete and heterogeneous data, and perform quantitative analysis of two major collections, EDH and EDCS.\"",\""process\"":\""Harvesting and cleaning over 580,000 records from EDH and EDCS, normalizing spatio-temporal and textual attributes, integrating datasets into machine-readable formats, and applying computational analysis to uncover trends.\"",\""results\"":\""Enabled the first side-by-side analysis of EDH and EDCS datasets, revealing spatio-temporal patterns in inscription habits and supporting collaborative research on inscriptions as proxies for societal dynamics.\""}"""
jXupS3QAeNgb.html;"""{\""goal\"":\""Exploring innovative digital publishing methods and the concept of scalable reading to create multilayered, interactive platforms for data-driven historical scholarship.\"",\""methods\"":\""Drawing inspiration from Robert Darnton's layered pyramid model and integrating tools like Jupyter Notebooks, Dataverse, and open-source technologies to develop a publication platform for digital hermeneutics.\"",\""process\"":\""Collaborating across disciplines to design an editorial platform, implementing scalable reading features for layered content navigation, and hosting data-driven publications with FAIR data principles for transparency and reproducibility.\"",\""results\"":\""Launched the Journal of Digital History, showcasing multilayered articles that combine narrative, data, and hermeneutics; created an innovative model for open-access digital publishing in historical research.\""}"""
4yxHGiqXYRbX.html;"""{\""goal\"":\""Building a representative newspaper corpus on return migration (1850-1950) using text mining methods to overcome bias and improve research accuracy in migration studies.\"",\""methods\"":\""Employing keyword searches, manual annotation, Latent Dirichlet Allocation (LDA), and Jensen-Shannon divergence to classify and refine the corpus into relevant and irrelevant articles.\"",\""process\"":\""Manually annotated 208 newspaper clippings, applied pre-processing techniques (tokenization, stemming), trained LDA models, and calculated topic distributions to identify dominant themes and classify documents.\"",\""results\"":\""Created a refined corpus of historical articles, improved the representativeness and relevance of data on return migration, and demonstrated the effectiveness of combining digital and traditional methods for corpus building.\""}"""
JJszM3GwAYDs.html;"""{\""goal\"":\""Analyzing the vocabulary of the crowd in Tacitus\u2019 works to uncover its semantic, sociological, and political dimensions using digital tools and quantitative methods.\"",\""methods\"":\""Employing lexicological analysis, manual annotation, and data visualization with tools such as Pandas, Matplotlib, and Seaborn to process occurrences of crowd-related terms in Tacitus\u2019 corpus.\"",\""process\"":\""Structured data from Tacitus\u2019 works into a CSV file with detailed annotations, excluded irrelevant occurrences, and used visualization to analyze the distributions and semantic nuances of terms like vulgus, turba, and multitudo.\"",\""results\"":\""Revealed vulgus as a distinct term representing politically active crowds, contrasting with turba and multitudo; demonstrated Tacitus\u2019 nuanced use of crowd vocabulary and its role in imperial political narratives.\""}"""